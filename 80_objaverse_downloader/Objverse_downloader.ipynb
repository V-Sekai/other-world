{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MIT License\n",
    "\n",
    "\n",
    "\n",
    " Copyright (c) 2024-present K. S. Ernest (iFire) Lee\n",
    "\n",
    "\n",
    "\n",
    " Copyright (c) 2024 Marcus Loren\n",
    "\n",
    "\n",
    "\n",
    " Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    " of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    " in the Software without restriction, including without limitation the rights\n",
    "\n",
    " to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    " copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    " furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    " The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    " copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    " IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    " FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    " AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    " LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    " OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    " SOFTWARE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Install the dependencies\n",
    "\n",
    "\n",
    "\n",
    " python3 -m pip install --break-system-packages --user requests tqdm trimesh pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Step 1 - Get model sizes & path\n",
    "\n",
    "\n",
    "\n",
    " Option 1 - Extract manually:\n",
    "\n",
    " 1. Run \"git clone https://huggingface.co/datasets/allenai/objaverse\" and then abort the command when it starts to download the models.\n",
    "\n",
    " 2. This will create a git repo folder, you then can run \"python dump_gitcommits.py > out.txt\" to dump the entire commit history\n",
    "\n",
    " 3. Then you call extract_models_from_dump(\"out.txt\") to parse and get all the model paths and their sizes.\n",
    "\n",
    "\n",
    "\n",
    " Option 2 - Use the pre-extracted json (model_sizes.json.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json \n",
    "import gzip\n",
    "\n",
    "def extract_models_from_dump(file_path):\n",
    "    model_sizes = {}\n",
    "    current_model = None\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Get model path\n",
    "            if \".glb\" in line:\n",
    "                # Extract model path\n",
    "                model_path = line.split()[-1].strip()\n",
    "                model_path = model_path.replace(\"b/\", \"\")\n",
    "                current_model = model_path\n",
    "            # Get current_model size\n",
    "            elif current_model and \"size\" in line: \n",
    "                \n",
    "                size = int(line.split()[-1].strip()) \n",
    "                model_sizes[current_model] = size \n",
    "                current_model = None\n",
    "    return model_sizes\n",
    " \n",
    " \n",
    " ## Option 1\n",
    "#model_sizes = extract_models_from_dump(\"out.txt\")  \n",
    "\n",
    "\n",
    "## Option 2\n",
    "with gzip.open(\"model_sizes.json.gz\", 'rb') as gzip_file: \n",
    "    model_sizes = json.loads(gzip_file.read().decode('utf-8'))\n",
    "    \n",
    "print(len(model_sizes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Download the meshes as per specified size limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm  \n",
    "from concurrent.futures import ThreadPoolExecutor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Download metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, folder_path, filename):\n",
    "    url = url + \"?download=true\"\n",
    "    print(url)\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # If the response was successful, no Exception will be raised\n",
    "        with open(os.path.join(folder_path, filename), 'wb') as f:\n",
    "            f.write(response.content) \n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(f\"Failed to download {filename}. Error: {err}\")\n",
    "        return False\n",
    "\n",
    "def download_metadata(base_url, save_dir, num_threads=6):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for i in range(1, 161):\n",
    "            filename = f\"000-{i:03d}.json.gz\"\n",
    "            file_url = base_url + filename\n",
    "            futures.append(executor.submit(download_file, file_url, save_dir, filename))\n",
    "\n",
    "        for future in tqdm(futures, total=len(futures)):\n",
    "            result = future.result()\n",
    "            if not result:\n",
    "                continue\n",
    "            \n",
    "base_url = \"https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/\" \n",
    "save_dir = './objaverse/metadata'\n",
    "os.makedirs(save_dir, exist_ok=True)   \n",
    "\n",
    "download_metadata(base_url, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Extract the metadata to a JSON with only the relevant information, e.g the models you downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 211\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    210\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m--> 211\u001b[0m \u001b[43mdownload_filtered_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminKb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m301\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxKb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40960\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxDownloadedMeshes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m   \n",
      "Cell \u001b[0;32mIn[27], line 167\u001b[0m, in \u001b[0;36mdownload_filtered_models\u001b[0;34m(model_sizes, filtered_json, base_url, save_dir, minKb, maxKb, num_threads, maxDownloadedMeshes)\u001b[0m\n\u001b[1;32m    163\u001b[0m filtered_models \u001b[38;5;241m=\u001b[39m {model_path: size \u001b[38;5;28;01mfor\u001b[39;00m model_path, size \u001b[38;5;129;01min\u001b[39;00m model_sizes\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m minKb \u001b[38;5;241m<\u001b[39m size \u001b[38;5;241m<\u001b[39m maxKb \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m}\n\u001b[1;32m    165\u001b[0m downloaded_meshes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 167\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiltered_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:238\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 238\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading: https://huggingface.co/datasets/allenai/objaverse/resolve/main/glbs/000-159/0592cf7990134513958a8b644c14672e.glb?download=true, 'utf-8' codec can't decode byte 0x8c in position 6528: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from pygltflib import GLTF2, BufferFormat\n",
    "from tqdm import tqdm\n",
    "\n",
    "captions_df = pd.read_csv('./objaverse_annotations/pali_captions.csv', sep=';')\n",
    "material_annotations_df = pd.read_csv('./objaverse_annotations/pali_material_annotations.csv', sep=';')\n",
    "type_annotations_df = pd.read_csv('./objaverse_annotations/pali_type_annotations.csv', sep=';')\n",
    "captions_dict = captions_df.set_index('object_uid').T.to_dict('list')\n",
    "material_annotations_dict = material_annotations_df.set_index('object_uid').T.to_dict('list')\n",
    "type_annotations_dict = type_annotations_df.set_index('object_uid').T.to_dict('list')\n",
    "\n",
    "existing_models = {}\n",
    "metadata = {}\n",
    "filtered_metadata = {}\n",
    "metadata_path = './objaverse/metadata'\n",
    "for file_name in os.listdir(metadata_path):\n",
    "    if file_name.endswith(\".gz\"):\n",
    "        file_path = os.path.join(metadata_path, file_name)\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            file_metadata = json.load(f)\n",
    "            metadata.update(file_metadata)\n",
    "\n",
    "input_directory = './objaverse/glbs'\n",
    "output_gltf_directory = './objaverse/gltf_xmp_json_ld'\n",
    "scaling_factor_constant = 0.95\n",
    "\n",
    "os.makedirs(output_gltf_directory, exist_ok=True)\n",
    "\n",
    "def convert_lists_to_ordered_xmp_format(data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, list):\n",
    "            # Always use '@list' to represent an ordered list.\n",
    "            data[key] = {'@list': value}\n",
    "        elif isinstance(value, dict):\n",
    "            convert_lists_to_ordered_xmp_format(value)\n",
    "\n",
    "def add_to_filtered_metadata(key, value):\n",
    "    if value is not None:\n",
    "        filtered_metadata[f\"vsekai:{key}\"] = value\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def download_model_convert_and_delete(model_url, gltf_path):\n",
    "    try:\n",
    "        response = requests.get(model_url)\n",
    "        if response.status_code != 200:\n",
    "            return\n",
    "        with tempfile.NamedTemporaryFile(mode='wb', delete=True) as temp:\n",
    "            temp.write(response.content)\n",
    "            temp.flush()\n",
    "            gltf = GLTF2().load(temp.name)\n",
    "            file_name, file_extension = os.path.splitext(file_path)\n",
    "            existing_models[os.path.basename(file_name)] = file_path\n",
    "            if not file_extension.lower() == \".glb\" and os.path.basename(file_name) in metadata:\n",
    "                return\n",
    "            gltf_file_path = os.path.join(output_gltf_directory, os.path.basename(file_name) + \".gltf\")\n",
    "            if os.path.exists(gltf_file_path):\n",
    "                return\n",
    "            gltf = GLTF2().load(file_path)\n",
    "            gltf.convert_buffers(BufferFormat.DATAURI)\n",
    "            gltf.save(gltf_file_path)\n",
    "            data = metadata[os.path.basename(file_name)]\n",
    "            if data[\"license\"] != \"by\":\n",
    "                os.remove(gltf_file_path)\n",
    "                return\n",
    "            \n",
    "            if not (100 <= data[\"faceCount\"] <= 2000):\n",
    "                os.remove(gltf_file_path)\n",
    "                return\n",
    "                \n",
    "            convert_lists_to_ordered_xmp_format(data)\n",
    "            filtered_metadata = {\n",
    "                \"@context\": {\n",
    "                    \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "                    \"vsekai\": \"http://v-sekai.org/vsekai/elements/0.4/\"\n",
    "                },\n",
    "                \"@id\": data[\"uid\"],\n",
    "                \"dc:title\": data[\"name\"],\n",
    "                \"dc:creator\": {\n",
    "                    \"@id\": data[\"user\"][\"uid\"],\n",
    "                    \"dc:name\": data[\"user\"][\"username\"]\n",
    "                },\n",
    "                \"dc:description\": data[\"description\"],\n",
    "                \"dc:date\": data[\"createdAt\"],\n",
    "                \"dc:identifier\": data[\"uri\"],\n",
    "                \"dc:source\": data[\"viewerUrl\"],\n",
    "                \"dc:rights\": data[\"license\"],\n",
    "                \"dc:subject\": data[\"tags\"],\n",
    "                \"dc:type\": \"3D Model\",\n",
    "                \"dc:relation\": data[\"user\"][\"profileUrl\"],\n",
    "                \"vsekai:viewCount\": data[\"viewCount\"],\n",
    "                \"vsekai:likeCount\": data[\"likeCount\"],\n",
    "                \"vsekai:commentCount\": data[\"commentCount\"],\n",
    "                \"vsekai:isDownloadable\": data[\"isDownloadable\"],\n",
    "                \"vsekai:publishedAt\": data[\"publishedAt\"],\n",
    "                \"vsekai:faceCount\": data[\"faceCount\"],\n",
    "                \"vsekai:vertexCount\": data[\"vertexCount\"],\n",
    "                \"vsekai:isAgeRestricted\": data[\"isAgeRestricted\"],\n",
    "            }\n",
    "\n",
    "            if data[\"uid\"] in captions_dict:\n",
    "                caption_annotation, caption_annotation_probability = captions_dict[data[\"uid\"]]\n",
    "                add_to_filtered_metadata(\"captionAnnotation\", caption_annotation)\n",
    "                add_to_filtered_metadata(\"captionAnnotationProbability\", caption_annotation_probability)\n",
    "\n",
    "            if data[\"uid\"] in material_annotations_dict:\n",
    "                material_annotation, material_annotation_probability = material_annotations_dict[data[\"uid\"]]\n",
    "                add_to_filtered_metadata(\"materialAnnotation\", material_annotation)\n",
    "                add_to_filtered_metadata(\"materialAnnotationProbability\", material_annotation_probability)\n",
    "\n",
    "            if data[\"uid\"] in type_annotations_dict:\n",
    "                type_annotation, type_annotation_probability = type_annotations_dict[data[\"uid\"]]\n",
    "                add_to_filtered_metadata(\"typeAnnotation\", type_annotation)\n",
    "                add_to_filtered_metadata(\"typeAnnotationProbability\", type_annotation_probability)\n",
    "\n",
    "            optional_tags = [\"animationCount\", \"staffpickedAt\", \"archives\", \"categories\"]\n",
    "            for tag in optional_tags:\n",
    "                if tag in data:\n",
    "                    add_to_filtered_metadata(tag, data[tag])\n",
    "\n",
    "            with open(gltf_file_path, 'r') as f:\n",
    "                gltf_json = json.load(f)\n",
    "\n",
    "            xmp_extension = {\n",
    "                \"KHR_xmp_json_ld\": {\n",
    "                    \"packets\": [filtered_metadata]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if 'extensions' in gltf_json['asset']:\n",
    "                if 'KHR_xmp_json_ld' in gltf_json['asset']['extensions']:\n",
    "                    gltf_json['asset']['extensions']['KHR_xmp_json_ld']['packets'].append(filtered_metadata)\n",
    "                else:\n",
    "                    gltf_json['asset']['extensions'].update(xmp_extension)\n",
    "            else:\n",
    "                gltf_json['asset']['extensions'] = xmp_extension\n",
    "\n",
    "            gltf_json['asset']['extensions']['KHR_xmp_json_ld']['packet'] = len(gltf_json['asset']['extensions']['KHR_xmp_json_ld']['packets']) - 1\n",
    "\n",
    "            if 'extensionsUsed' in gltf_json:\n",
    "                if \"KHR_xmp_json_ld\" not in gltf_json['extensionsUsed']:\n",
    "                    gltf_json['extensionsUsed'].append(\"KHR_xmp_json_ld\")\n",
    "            else:\n",
    "                gltf_json['extensionsUsed'] = [\"KHR_xmp_json_ld\"]\n",
    "\n",
    "            with open(gltf_file_path, 'w') as f:\n",
    "                json.dump(gltf_json, f, indent=4)\n",
    "\n",
    "            # Save the converted GLTF file\n",
    "            gltf.save(gltf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading: {model_url}, {e}\")\n",
    "\n",
    "def download_filtered_models(model_sizes, filtered_json, base_url, save_dir, minKb, maxKb, num_threads = 6, maxDownloadedMeshes = 1):\n",
    "    filtered_models = {model_path: size for model_path, size in model_sizes.items() if minKb < size < maxKb * 1024}\n",
    "\n",
    "    downloaded_meshes = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for model_path, size in filtered_models.items():\n",
    "            if downloaded_meshes >= maxDownloadedMeshes:\n",
    "                break\n",
    "\n",
    "            if model_path in filtered_json:\n",
    "                continue\n",
    "            \n",
    "            folder_name = os.path.dirname(model_path)\n",
    "            sub_folder = os.path.join(save_dir, folder_name)\n",
    "            os.makedirs(sub_folder, exist_ok=True)\n",
    "            \n",
    "            file_name = os.path.basename(model_path)\n",
    "            save_path = os.path.join(sub_folder, file_name)\n",
    "\n",
    "            if file_name in model_sizes:\n",
    "                print(\"The file is filtered from this dataset.\")\n",
    "                continue\n",
    "            \n",
    "            if not os.path.exists(save_path):\n",
    "                model_url = f\"{base_url}/{model_path}?download=true\"              \n",
    "                futures.append(executor.submit(download_model_convert_and_delete, model_url, save_path))\n",
    "                \n",
    "                downloaded_meshes += 1\n",
    "                \n",
    "    for future in tqdm(futures, total=len(futures)):\n",
    "        future.result()\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/allenai/objaverse/resolve/main\"  \n",
    "save_dir = f'./objaverse' \n",
    "\n",
    "json_file_path = \"filtered_face_count.json\"\n",
    "filtered_json = {}\n",
    "\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        filtered_json = json.load(f)\n",
    "else:\n",
    "    print(f'File {json_file_path} does not exist.')\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True) \n",
    "download_filtered_models(model_sizes, filtered_json, base_url, save_dir, minKb = 301, maxKb = 40960, num_threads = 24, maxDownloadedMeshes = 1000)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
