{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MIT License\n",
    "\n",
    "\n",
    "\n",
    " Copyright (c) 2024-present K. S. Ernest (iFire) Lee\n",
    "\n",
    "\n",
    "\n",
    " Copyright (c) 2024 Marcus Loren\n",
    "\n",
    "\n",
    "\n",
    " Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    " of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    " in the Software without restriction, including without limitation the rights\n",
    "\n",
    " to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    " copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    " furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    " The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    " copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    " IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    " FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    " AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    " LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    " OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    " SOFTWARE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Install the dependencies\n",
    "\n",
    "\n",
    "\n",
    " python3 -m pip install --break-system-packages --user requests tqdm trimesh pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Step 1 - Get model sizes & path\n",
    "\n",
    "\n",
    "\n",
    " Option 1 - Extract manually:\n",
    "\n",
    " 1. Run \"git clone https://huggingface.co/datasets/allenai/objaverse\" and then abort the command when it starts to download the models.\n",
    "\n",
    " 2. This will create a git repo folder, you then can run \"python dump_gitcommits.py > out.txt\" to dump the entire commit history\n",
    "\n",
    " 3. Then you call extract_models_from_dump(\"out.txt\") to parse and get all the model paths and their sizes.\n",
    "\n",
    "\n",
    "\n",
    " Option 2 - Use the pre-extracted json (model_sizes.json.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798759\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json \n",
    "import gzip\n",
    "\n",
    "def extract_models_from_dump(file_path):\n",
    "    model_sizes = {}\n",
    "    current_model = None\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Get model path\n",
    "            if \".glb\" in line:\n",
    "                # Extract model path\n",
    "                model_path = line.split()[-1].strip()\n",
    "                model_path = model_path.replace(\"b/\", \"\")\n",
    "                current_model = model_path\n",
    "            # Get current_model size\n",
    "            elif current_model and \"size\" in line: \n",
    "                \n",
    "                size = int(line.split()[-1].strip()) \n",
    "                model_sizes[current_model] = size \n",
    "                current_model = None\n",
    "    return model_sizes\n",
    " \n",
    " \n",
    " ## Option 1\n",
    "#model_sizes = extract_models_from_dump(\"out.txt\")  \n",
    "\n",
    "\n",
    "## Option 2\n",
    "with gzip.open(\"model_sizes.json.gz\", 'rb') as gzip_file: \n",
    "    model_sizes = json.loads(gzip_file.read().decode('utf-8'))\n",
    "    \n",
    "print(len(model_sizes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Download the meshes as per specified size limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm  \n",
    "from concurrent.futures import ThreadPoolExecutor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Download metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-001.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-002.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-003.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-004.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-005.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-006.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/160 [00:00<00:31,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-007.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-008.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-009.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 4/160 [00:00<00:10, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-010.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-011.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-012.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-013.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-014.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 15/160 [00:00<00:04, 34.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-015.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-016.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-017.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-018.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-019.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-020.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-021.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-022.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-023.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 19/160 [00:00<00:03, 35.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-024.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-025.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-026.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-027.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-028.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-029.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-030.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-031.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 28/160 [00:00<00:04, 32.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-032.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-033.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-034.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-035.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-036.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-037.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-038.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-039.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 34/160 [00:01<00:03, 35.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-040.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-041.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-042.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-043.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-044.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-045.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 38/160 [00:01<00:04, 27.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-046.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-047.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-048.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-049.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-050.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-051.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-052.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-053.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-054.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 45/160 [00:01<00:04, 28.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-055.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-056.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-057.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-058.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-059.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-060.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-061.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 62/160 [00:01<00:02, 38.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-062.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-063.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-064.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-065.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-066.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-067.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-068.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-069.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-070.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 68/160 [00:02<00:02, 39.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-071.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-072.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-073.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-074.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-075.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-076.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-077.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-078.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 78/160 [00:02<00:02, 34.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-079.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-080.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-081.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-082.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-083.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-084.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-085.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 82/160 [00:02<00:02, 34.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-086.json.gz?download=truehttps://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-087.json.gz?download=true\n",
      "\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-088.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-089.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-090.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-091.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-092.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-093.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-094.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-095.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-096.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-097.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-098.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-099.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-100.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-101.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-102.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-103.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-104.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-105.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-106.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-107.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-108.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-109.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-110.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-111.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-112.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 86/160 [00:03<00:06, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-113.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-114.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-115.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-116.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-117.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-118.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-119.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 113/160 [00:03<00:01, 34.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-120.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-121.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-122.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-123.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-124.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-125.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-126.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 128/160 [00:04<00:00, 39.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-127.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-128.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-129.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-130.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-131.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-132.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-133.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-134.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-135.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 134/160 [00:04<00:00, 35.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-136.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-137.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-138.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-139.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-140.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-141.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-142.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 145/160 [00:04<00:00, 38.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-143.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-144.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-145.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-146.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-147.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-148.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-149.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-150.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-151.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 150/160 [00:04<00:00, 37.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-152.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-153.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-154.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-155.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-156.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-157.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-158.json.gz?download=true\n",
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-159.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 155/160 [00:04<00:00, 39.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-160.json.gz?download=true\n",
      "Failed to download 000-160.json.gz. Error: 404 Client Error: Not Found for url: https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/000-160.json.gz?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:05<00:00, 31.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, folder_path, filename):\n",
    "    url = url + \"?download=true\"\n",
    "    print(url)\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # If the response was successful, no Exception will be raised\n",
    "        with open(os.path.join(folder_path, filename), 'wb') as f:\n",
    "            f.write(response.content) \n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(f\"Failed to download {filename}. Error: {err}\")\n",
    "        return False\n",
    "\n",
    "def download_metadata(base_url, save_dir, num_threads=6):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for i in range(1, 161):\n",
    "            filename = f\"000-{i:03d}.json.gz\"\n",
    "            file_url = base_url + filename\n",
    "            futures.append(executor.submit(download_file, file_url, save_dir, filename))\n",
    "\n",
    "        for future in tqdm(futures, total=len(futures)):\n",
    "            result = future.result()\n",
    "            if not result:\n",
    "                continue\n",
    "            \n",
    "base_url = \"https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/\" \n",
    "save_dir = './objaverse/metadata'\n",
    "os.makedirs(save_dir, exist_ok=True)   \n",
    "\n",
    "download_metadata(base_url, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Extract the metadata to a JSON with only the relevant information, e.g the models you downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xcc in position 12: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 201\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    199\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m--> 201\u001b[0m \u001b[43mdownload_filtered_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminKb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m301\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxKb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40960\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxDownloadedMeshes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m   \n",
      "Cell \u001b[0;32mIn[10], line 185\u001b[0m, in \u001b[0;36mdownload_filtered_models\u001b[0;34m(model_sizes, filtered_json, base_url, save_dir, minKb, maxKb, num_threads, maxDownloadedMeshes)\u001b[0m\n\u001b[1;32m    182\u001b[0m             downloaded_meshes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(futures, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(futures)):\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[10], line 57\u001b[0m, in \u001b[0;36mdownload_model_convert_and_delete\u001b[0;34m(model_url, gltf_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m temp\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     56\u001b[0m temp\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m---> 57\u001b[0m gltf \u001b[38;5;241m=\u001b[39m \u001b[43mGLTF2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m gltf\u001b[38;5;241m.\u001b[39mconvert_buffers(BufferFormat\u001b[38;5;241m.\u001b[39mDATAURI)\n\u001b[1;32m     59\u001b[0m data \u001b[38;5;241m=\u001b[39m metadata[os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_name)]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/pygltflib/__init__.py:1182\u001b[0m, in \u001b[0;36mGLTF2.load\u001b[0;34m(cls, fname)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_binary(fname)\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m   1184\u001b[0m obj\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/pygltflib/__init__.py:1127\u001b[0m, in \u001b[0;36mGLTF2.load_json\u001b[0;34m(cls, fname)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(\u001b[38;5;28mcls\u001b[39m, fname):\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 1127\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mgltf_from_json(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xcc in position 12: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from pygltflib import GLTF2, BufferFormat\n",
    "from tqdm import tqdm\n",
    "\n",
    "captions_df = pd.read_csv('./objaverse_annotations/pali_captions.csv', sep=';')\n",
    "material_annotations_df = pd.read_csv('./objaverse_annotations/pali_material_annotations.csv', sep=';')\n",
    "type_annotations_df = pd.read_csv('./objaverse_annotations/pali_type_annotations.csv', sep=';')\n",
    "captions_dict = captions_df.set_index('object_uid').T.to_dict('list')\n",
    "material_annotations_dict = material_annotations_df.set_index('object_uid').T.to_dict('list')\n",
    "type_annotations_dict = type_annotations_df.set_index('object_uid').T.to_dict('list')\n",
    "\n",
    "existing_models = {}\n",
    "metadata = {}\n",
    "filtered_metadata = {}\n",
    "metadata_path = './objaverse/metadata'\n",
    "for file_name in os.listdir(metadata_path):\n",
    "    if file_name.endswith(\".gz\"):\n",
    "        file_path = os.path.join(metadata_path, file_name)\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            file_metadata = json.load(f)\n",
    "            metadata.update(file_metadata)\n",
    "\n",
    "input_directory = './objaverse/glbs'\n",
    "output_gltf_directory = './objaverse/gltf_xmp_json_ld'\n",
    "scaling_factor_constant = 0.95\n",
    "\n",
    "os.makedirs(output_gltf_directory, exist_ok=True)\n",
    "\n",
    "def convert_lists_to_ordered_xmp_format(data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, list):\n",
    "            # Always use '@list' to represent an ordered list.\n",
    "            data[key] = {'@list': value}\n",
    "        elif isinstance(value, dict):\n",
    "            convert_lists_to_ordered_xmp_format(value)\n",
    "\n",
    "def add_to_filtered_metadata(key, value):\n",
    "    if value is not None:\n",
    "        filtered_metadata[f\"vsekai:{key}\"] = value\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def download_model_convert_and_delete(model_url, gltf_path):\n",
    "    response = requests.get(model_url)\n",
    "    if response.status_code != 200:\n",
    "        return\n",
    "    with tempfile.NamedTemporaryFile(mode='wb', delete=True) as temp:\n",
    "        temp.write(response.content)\n",
    "        temp.flush()\n",
    "        gltf = GLTF2().load(temp.name)\n",
    "        gltf.convert_buffers(BufferFormat.DATAURI)\n",
    "        # data = metadata[os.path.basename(file_name)]\n",
    "        # if data[\"license\"] != \"by\":\n",
    "        #     return\n",
    "        \n",
    "        # if not (100 <= data[\"faceCount\"] <= 2000):\n",
    "        #     return\n",
    "            \n",
    "        # convert_lists_to_ordered_xmp_format(data)\n",
    "        # filtered_metadata = {\n",
    "        #     \"@context\": {\n",
    "        #         \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "        #         \"vsekai\": \"http://v-sekai.org/vsekai/elements/0.4/\"\n",
    "        #     },\n",
    "        #     \"@id\": data[\"uid\"],\n",
    "        #     \"dc:title\": data[\"name\"],\n",
    "        #     \"dc:creator\": {\n",
    "        #         \"@id\": data[\"user\"][\"uid\"],\n",
    "        #         \"dc:name\": data[\"user\"][\"username\"]\n",
    "        #     },\n",
    "        #     \"dc:description\": data[\"description\"],\n",
    "        #     \"dc:date\": data[\"createdAt\"],\n",
    "        #     \"dc:identifier\": data[\"uri\"],\n",
    "        #     \"dc:source\": data[\"viewerUrl\"],\n",
    "        #     \"dc:rights\": data[\"license\"],\n",
    "        #     \"dc:subject\": data[\"tags\"],\n",
    "        #     \"dc:type\": \"3D Model\",\n",
    "        #     \"dc:relation\": data[\"user\"][\"profileUrl\"],\n",
    "        #     \"vsekai:viewCount\": data[\"viewCount\"],\n",
    "        #     \"vsekai:likeCount\": data[\"likeCount\"],\n",
    "        #     \"vsekai:commentCount\": data[\"commentCount\"],\n",
    "        #     \"vsekai:isDownloadable\": data[\"isDownloadable\"],\n",
    "        #     \"vsekai:publishedAt\": data[\"publishedAt\"],\n",
    "        #     \"vsekai:faceCount\": data[\"faceCount\"],\n",
    "        #     \"vsekai:vertexCount\": data[\"vertexCount\"],\n",
    "        #     \"vsekai:isAgeRestricted\": data[\"isAgeRestricted\"],\n",
    "        # }\n",
    "\n",
    "        # if data[\"uid\"] in captions_dict:\n",
    "        #     caption_annotation, caption_annotation_probability = captions_dict[data[\"uid\"]]\n",
    "        #     add_to_filtered_metadata(\"captionAnnotation\", caption_annotation)\n",
    "        #     add_to_filtered_metadata(\"captionAnnotationProbability\", caption_annotation_probability)\n",
    "\n",
    "        # if data[\"uid\"] in material_annotations_dict:\n",
    "        #     material_annotation, material_annotation_probability = material_annotations_dict[data[\"uid\"]]\n",
    "        #     add_to_filtered_metadata(\"materialAnnotation\", material_annotation)\n",
    "        #     add_to_filtered_metadata(\"materialAnnotationProbability\", material_annotation_probability)\n",
    "\n",
    "        # if data[\"uid\"] in type_annotations_dict:\n",
    "        #     type_annotation, type_annotation_probability = type_annotations_dict[data[\"uid\"]]\n",
    "        #     add_to_filtered_metadata(\"typeAnnotation\", type_annotation)\n",
    "        #     add_to_filtered_metadata(\"typeAnnotationProbability\", type_annotation_probability)\n",
    "\n",
    "        # optional_tags = [\"animationCount\", \"staffpickedAt\", \"archives\", \"categories\"]\n",
    "        # for tag in optional_tags:\n",
    "        #     if tag in data:\n",
    "        #         add_to_filtered_metadata(tag, data[tag])\n",
    "\n",
    "        # file_name, _file_extension = os.path.splitext(file_path)\n",
    "        # existing_models[os.path.basename(file_name)] = file_path\n",
    "        # gltf_file_path = os.path.join(output_gltf_directory, os.path.basename(file_name) + \".gltf\")\n",
    "        \n",
    "        with open(gltf_file_path, 'r') as f:\n",
    "            gltf_json = json.load(f)\n",
    "\n",
    "        # xmp_extension = {\n",
    "        #     \"KHR_xmp_json_ld\": {\n",
    "        #         \"packets\": [filtered_metadata]\n",
    "        #     }\n",
    "        # }\n",
    "\n",
    "        # if 'extensions' in gltf_json['asset']:\n",
    "        #     if 'KHR_xmp_json_ld' in gltf_json['asset']['extensions']:\n",
    "        #         gltf_json['asset']['extensions']['KHR_xmp_json_ld']['packets'].append(filtered_metadata)\n",
    "        #     else:\n",
    "        #         gltf_json['asset']['extensions'].update(xmp_extension)\n",
    "        # else:\n",
    "        #     gltf_json['asset']['extensions'] = xmp_extension\n",
    "\n",
    "        # gltf_json['asset']['extensions']['KHR_xmp_json_ld']['packet'] = len(gltf_json['asset']['extensions']['KHR_xmp_json_ld']['packets']) - 1\n",
    "\n",
    "        # if 'extensionsUsed' in gltf_json:\n",
    "        #     if \"KHR_xmp_json_ld\" not in gltf_json['extensionsUsed']:\n",
    "        #         gltf_json['extensionsUsed'].append(\"KHR_xmp_json_ld\")\n",
    "        # else:\n",
    "        #     gltf_json['extensionsUsed'] = [\"KHR_xmp_json_ld\"]\n",
    "\n",
    "        with open(gltf_file_path, 'w') as f:\n",
    "            json.dump(gltf_json, f, indent=4)\n",
    "\n",
    "\n",
    "def download_filtered_models(model_sizes, filtered_json, base_url, save_dir, minKb, maxKb, num_threads = 6, maxDownloadedMeshes = 1):\n",
    "    filtered_models = {model_path: size for model_path, size in model_sizes.items() if minKb < size < maxKb * 1024}\n",
    "\n",
    "    downloaded_meshes = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for model_path, size in filtered_models.items():\n",
    "            if downloaded_meshes >= maxDownloadedMeshes:\n",
    "                break\n",
    "\n",
    "            if model_path in filtered_json:\n",
    "                continue\n",
    "            \n",
    "            folder_name = os.path.dirname(model_path)\n",
    "            sub_folder = os.path.join(save_dir, folder_name)\n",
    "            os.makedirs(sub_folder, exist_ok=True)\n",
    "            \n",
    "            file_name = os.path.basename(model_path)\n",
    "            save_path = os.path.join(sub_folder, file_name)\n",
    "\n",
    "            if file_name in model_sizes:\n",
    "                print(\"The file is filtered from this dataset.\")\n",
    "                continue\n",
    "            \n",
    "            if not os.path.exists(save_path):\n",
    "                model_url = f\"{base_url}/{model_path}?download=true\"\n",
    "                try:    \n",
    "                    futures.append(executor.submit(download_model_convert_and_delete, model_url, save_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading: {model_url}, {e}\")        \n",
    "                downloaded_meshes += 1\n",
    "                \n",
    "    for future in tqdm(futures, total=len(futures)):\n",
    "        future.result()\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/allenai/objaverse/resolve/main\"  \n",
    "save_dir = f'./objaverse' \n",
    "\n",
    "json_file_path = \"filtered_face_count.json\"\n",
    "filtered_json = {}\n",
    "\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        filtered_json = json.load(f)\n",
    "else:\n",
    "    print(f'File {json_file_path} does not exist.')\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True) \n",
    "\n",
    "download_filtered_models(model_sizes, filtered_json, base_url, save_dir, minKb = 301, maxKb = 40960, num_threads = 24, maxDownloadedMeshes = 100)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
